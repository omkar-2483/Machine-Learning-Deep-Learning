{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "007be80c-733f-4ec4-af50-e07b49da2bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03e9b933-3d13-40c4-9c00-933fce5b7101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47a17f19-74eb-4599-bea0-acb407d2b4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e75aaf9-ffef-499a-8b20-3f8a52d95b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = 'api key reference here'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a19d055-9443-41e9-b864-ab08934ca544",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gpt_response(message, max_tokens=200):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, \n",
    "                  {\"role\": \"user\", \"content\": message}],\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    return response['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee14331a-6372-4100-97ea-1f96533dfa70",
   "metadata": {},
   "source": [
    "Model: OpenAI API - gpt-3.5-turbo\n",
    "\n",
    "This code defines a function called get_gpt_response that takes in a message and an optional argument max_tokens. The function makes use of the OpenAI API to generate a response based on the given message.\n",
    "\n",
    "## Here is a line-by-line explanation of the code in the get_gpt_response function:\n",
    "\n",
    "1. def get_gpt_response(message, max_tokens=200): - This line defines the function get_gpt_response with two parameters: message and max_tokens, which has a default value of 200.\n",
    "\n",
    "2. response = openai.ChatCompletion.create( - This line creates a chat completion request using the OpenAI API.\n",
    "\n",
    "3. model=\"gpt-3.5-turbo\", - This specifies the model to use for generating the response. In this case, it uses the \"gpt-3.5-turbo\" model.\n",
    "\n",
    "4. messages=[{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, - This provides a list of messages to the API. The first message, with the role \"system\", provides a prompt that sets the system-level instruction.\n",
    "\n",
    "5. {\"role\": \"user\", \"content\": message}], - This adds a user message to the list of messages. The content of this message is the input message parameter passed into the function.\n",
    "\n",
    "6. max_tokens=max_tokens, - This sets the maximum number of tokens to generate in the response. By default, it's set to 200, but it can be overridden by providing a different value for the max_tokens parameter when calling the function.\n",
    "\n",
    "7. temperature=0.7 - This sets the \"temperature\" for the response generation, controlling the randomness of the output. A higher value (e.g., 0.7) leads to more random responses, while a lower value (e.g., 0.2) makes the responses more focused and deterministic.\n",
    "\n",
    "8. ) - The closing parentheses for the openai.ChatCompletion.create function call.\n",
    "\n",
    "9. return response['choices'][0]['message']['content'] - This line returns the generated response from the API call. The response is accessed using dictionary-like indexing to retrieve the content of the message.\n",
    "\n",
    "In the provided code, the get_gpt_response function is called multiple times with different prompts, and the generated response is printed using the print function. However, since the STDOUT and result of the focal cell are not provided, I cannot tell what the exact output would be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e0a5ce0-6c0c-40d6-903a-b635249fe602",
   "metadata": {},
   "outputs": [
    {
     "ename": "APIRemovedInV1",
     "evalue": "\n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAPIRemovedInV1\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m promt\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwhat is chatgpt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 2\u001b[0m response\u001b[38;5;241m=\u001b[39m  \u001b[43mget_gpt_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpromt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "Cell \u001b[1;32mIn[13], line 2\u001b[0m, in \u001b[0;36mget_gpt_response\u001b[1;34m(message, max_tokens)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_gpt_response\u001b[39m(message, max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m):\n\u001b[1;32m----> 2\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-3.5-turbo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msystem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mYou are a helpful assistant.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m                  \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.7\u001b[39;49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openai\\lib\\_old_api.py:39\u001b[0m, in \u001b[0;36mAPIRemovedInV1Proxy.__call__\u001b[1;34m(self, *_args, **_kwargs)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m_args: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_kwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m---> 39\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m APIRemovedInV1(symbol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_symbol)\n",
      "\u001b[1;31mAPIRemovedInV1\u001b[0m: \n\nYou tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n"
     ]
    }
   ],
   "source": [
    "promt= 'what is chatgpt'\n",
    "response=  get_gpt_response(promt)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
